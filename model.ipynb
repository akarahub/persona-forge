{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# YAML config\n",
    "try:\n",
    "    with open(r\".\\config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "# Logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s\",\n",
    "    filename=config[\"log_dir\"] +\n",
    "    f\"{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log\",\n",
    "    filemode=\"w\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Config file and logger setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preprocessing\n",
    "def load_and_preprocess_data(images_path, size):\n",
    "    \"\"\"Loads, resizes, and normalizes Pokemon images.\"\"\"\n",
    "    try:\n",
    "        images = []\n",
    "        for f in os.listdir(images_path):\n",
    "            f = os.path.join(images_path, f)\n",
    "            try:\n",
    "                img = Image.open(f).resize(size)\n",
    "                img_array = np.array(img).astype(\n",
    "                    \"float32\") / 255.0  # Normalize to [0, 1]\n",
    "                images.append(img_array)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error loading or processing image {f}: {e}\")\n",
    "\n",
    "        return np.array(images)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading images: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_and_preprocess_data(config[\"data_path\"], config[\"size\"])\n",
    "images_count = len(images)\n",
    "\n",
    "X_train = images[:int(images_count*0.8)]\n",
    "X_test = images[int(images_count*0.8):]\n",
    "\n",
    "N, H, W = X_train.shape[:-1]\n",
    "D = H * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the data\n",
    "X_train = X_train.reshape(-1, D)\n",
    "X_test = X_test.reshape(-1, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models Definitions\n",
    "def build_generator(latent_dim):\n",
    "    try:\n",
    "        i = Input(shape=(latent_dim,))\n",
    "        x = Dense(256, activation=LeakyReLU(negative_slope=0.2))(i)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Dense(512, activation=LeakyReLU(negative_slope=0.2))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Dense(1024, activation=LeakyReLU(negative_slope=0.2))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Dense(D, activation=\"tanh\")(x)  # -1 and 1 values used\n",
    "\n",
    "        model = Model(i, x)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Generator failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_discriminator(img_size):\n",
    "    try:\n",
    "        \"\"\"Builds the discriminator model.\"\"\"\n",
    "        i = Input(shape=(img_size,))\n",
    "        x = Dense(512, activation=LeakyReLU(negative_slope=0.2))(i)\n",
    "        x = Dense(256, activation=LeakyReLU(negative_slope=0.2))(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(i, x)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n",
    "            learning_rate=0.0002, beta_1=0.5, beta_2=0.99, epsilon=1e-8), metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Discriminator failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(D)\n",
    "generator = build_generator(config[\"latent_dimansinality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models(generator, discriminator):\n",
    "    try:\n",
    "        \"\"\"Combines the generator and discriminator.\"\"\"\n",
    "        # Create an input to represent noise sample from latent space\n",
    "        z = Input(shape=(config[\"latent_dimansinality\"],))\n",
    "\n",
    "        # Pass noise to get image\n",
    "        img = generator(z)\n",
    "\n",
    "        # Making sure only generator is trainable\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        # True output is fake yet we label as real\n",
    "        fake_pred = discriminator(img)\n",
    "\n",
    "        combined_model = Model(z, fake_pred)\n",
    "        combined_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(\n",
    "            learning_rate=0.0002, beta_1=0.5, beta_2=0.99, epsilon=1e-8))\n",
    "\n",
    "        return combined_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Combining models failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = combine_models(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def sample_image(epochs):\n",
    "    \"\"\"Generates and saves sample images at specified epochs.\"\"\"\n",
    "    generator = build_generator(config[\"latent_dimansinality\"])\n",
    "\n",
    "    rows, cols = 5, 5\n",
    "    noise = np.random.randn(rows * cols, config[\"latent_dimansinality\"])\n",
    "    imgs = generator.predict(noise)\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    idx = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i, j].imshow(imgs[idx].reshape(H, W), cmap=\"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "            idx += 1\n",
    "    fig.savefig(r\".\\%d.png\" % epochs)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gan(generator, discriminator, X_train):\n",
    "    \"\"\"Validates the GAN by generating and displaying samples.\"\"\"\n",
    "    # Generate a batch of samples\n",
    "    noise = np.random.randn(\n",
    "        config[\"batch_size\"], config[\"latent_dimansinality\"])\n",
    "    generated_images = generator.predict(noise)\n",
    "\n",
    "    # Display the generated images\n",
    "    fig, axes = plt.subplots(\n",
    "        config[\"batch_size\"], 1, figsize=(8, 4 * config[\"batch_size\"]))\n",
    "    for i in range(config[\"batch_size\"]):\n",
    "        axes[i].imshow(generated_images[i].reshape(H, W), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_gan(discriminator, combined_model, X_train, epochs, batch_size):\n",
    "    \"\"\"Trains the GAN.\"\"\"\n",
    "    try:\n",
    "        ones = np.ones(batch_size)\n",
    "        zeros = np.zeros(batch_size)\n",
    "\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Real images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            real_imgs = X_train[idx]\n",
    "\n",
    "            # Generate fake images\n",
    "            noise = np.random.randn(batch_size, config[\"latent_dimansinality\"])\n",
    "            fake_images = generator.predict(noise)\n",
    "\n",
    "            # Train discriminator\n",
    "            d_loss_real, d_acc_real = discriminator.train_on_batch(\n",
    "                real_imgs, ones)\n",
    "            d_loss_fake, d_acc_fake = discriminator.train_on_batch(\n",
    "                fake_images, zeros)\n",
    "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.randn(batch_size, config[\"latent_dimansinality\"])\n",
    "            g_loss = combined_model.train_on_batch(noise, ones)\n",
    "\n",
    "            # Validation\n",
    "            # if epoch % validation_frequency == 0:\n",
    "            #     validate_gan(generator, discriminator, X_train) # Calls validation function\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\")\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch}, D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "            if epoch % config[\"sample_period\"] == 0:\n",
    "                sample_image(epoch)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(discriminator, combined_model, X_train,\n",
    "          config[\"epochs\"], config[\"batch_size\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOkZKH4FefmTkDaljfFyR2U",
   "gpuType": "T4",
   "mount_file_id": "1_soMJETgNbkordnp2Vu4UgiZJoaUYiY5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
